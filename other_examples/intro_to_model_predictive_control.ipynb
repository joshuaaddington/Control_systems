{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Model Predictive Control (BYU ME 431 / ECE 483)\n",
    "\n",
    "This intro is built on a tutorial written by Mathew Haskell, and modified by Dr. Killpack. This is meant as a simple introduction to MPC, with some practical experience with tuning the weights and understanding the effect of certain parameters assuming the following: \n",
    "\n",
    "* quadratic cost\n",
    "* linear and discrete-time dynamics\n",
    "* additional constraints on states or inputs are linear \n",
    "* we are computationally limited (so length of time horizon matters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPC overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MPC is a form of optimal control that can include constraints on states, inputs, or functions of either (in addition to using the normal dynamics constraint to keep solutions physically meaningful or feasible). The picture below shows the main ideas that are necessary to understand MPC. \n",
    "\n",
    "<img src=\"./images/MPC.png\" alt=\"drawing\" width=50% />\n",
    "\n",
    "The basic idea behind MPC is that an optimization problem is solved to generate a trajectory of inputs to apply to a system over a specified time horizon (aquamarine line).\n",
    "These inputs are applied to an internal model in order to predict the future trajectory of states over the same time horizon (gold line) that drive the system towards its goal trajectory (red line).\n",
    "\n",
    "Generally, only the first calculated input from the horizon is applied to the system (aquamarine line from k to k+1) and the problem is repeatedly solved at each time step (k+1 in this figure becomes k at the next time step), so there is a rolling time window or \"horizon.\" This is why MPC is also referred to as receeding horizon control.\n",
    "\n",
    "This allows for the benefit of feedback more frequently and can make up for some modeling errors, essentially approximating or giving the benefits of normal feedback control or LQR where we found a gain matrix. \n",
    "\n",
    "MPC has three main pieces:\n",
    "  1. Model\n",
    "  1. Time Horizon\n",
    "  1. Optimization (constraints and objective)\n",
    "\n",
    "We will take a look at each of these below. There is a table of contents menu on the left sidebar (or after you click \"outline\" at the top of the notebook) to quickly jump to a section. Also, you can collapse/expand a cell or group of cells by clicking the arrow next to the heading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name implies, MPC requires a model that predicts future states based on the current state and provided input.\n",
    "Some algorithms use continuous dynamic models, but essentially perform numerical integration as part of the optimization. Instead, in this tutorial, we propagate the dynamics with a discrete-time model. \n",
    "\n",
    "A common way to get a discrete-time model is to take the continuous-time state space form of a dynamic system:\n",
    "\n",
    "$$\\dot{x}(t)=f(x(t),u(t))$$\n",
    "and then discretize it:\n",
    "$$x_{k+1}=f_d(x_k,u_k).$$\n",
    "\n",
    "An example of this process for linear dynamics is shown below in the mass-spring-damper example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear vs Nonlinear MPC\n",
    "\n",
    "At this point, we should distinguish between linear and nonlinear MPC. The literature on MPC is not always consistent on what the difference is between linear MPC and nonlinear MPC (NMPC). In this tutorial, we will make that distiction based on whether the model is linear or nonlinear throughout the optimization. This means if a nonlinear model is linearized in some way during the optimization over a single time horizon, we consider it linear MPC even though the system has nonlinear dynamics. \n",
    "\n",
    "One could call this \"linearized MPC,\" and this is similar to LQR (*linear* quadratic regulator) where you can perform LQR on a nonlinear system by linearizing, but it is still called a *linear* control method. \n",
    "\n",
    "The distinction we are making in this tutorial is that we will only refer to NMPC if the optimization problem is using nonlinear dynamics to propagate the state over the time horizon. As you can imagine, using nonlinear dynamics during the optimization may give more accurate results, but requires much more computational effort and takes more time to solve. It is common in the literature to use linear MPC if you need a lot of speed. It is also a gentler introduction and is therefore the focus of this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linearization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linearization can be done around any solution to the system, but the simplest linearization point is an equilibrium point.\n",
    "\n",
    "A system is in equilibrium when $\\dot{x}_{eq} = f(x_{eq},u_{eq}) = 0$, meaning that the states will not change from equilibrium when the equilibrium input is applied. Linearizing equations in a general nonlinear state-space form around some equilibrium point $(x_{eq}, u_{eq})$ allows you to generate a linear state-space model with $\\tilde{x}=x-x_{eq}$ and $\\tilde{u}=u-u_{eq}$.\n",
    "\n",
    "We can accomplish this using a mulitivariate Taylor series expansion (i.e. Jacobian linearization):\n",
    "\n",
    "$\\begin{array}{rcl}\n",
    "\\dot{x}(t) &\\approx& f(x_{eq},u_{eq}) + \\left.\\frac{\\partial f}{\\partial x}\\right\\vert_{eq}\\left(x(t)-x_{eq}\\right) + \\left.\\frac{\\partial f}{\\partial u}\\right\\vert_{eq}\\left(u(t)-u_{eq}\\right) \\\\\n",
    "\\dot{\\tilde{x}}(t) &=& \\left.\\frac{\\partial f}{\\partial x}\\right\\vert_{eq}\\tilde{x}(t) + \\left.\\frac{\\partial f}{\\partial u}\\right\\vert_{eq}\\tilde{u}(t) \\\\\n",
    "&=& A\\tilde{x}(t)+B\\tilde{u}(t).\n",
    "\\end{array}$\n",
    "\n",
    "This gives us the model derived earlier in chapters 4 and 6 of our textbook. \n",
    "\n",
    "$$\\dot{\\tilde{x}}(t) = A\\tilde{x}(t)+B\\tilde{u}(t).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above model, we can perform a simple forward difference numeric integration in order to discretize it (this is an overly simple discretization method that is not used in the example below, but is used to show the general idea of discretization of a state space model). This is accomplished by assuming we can approximate $\\dot{\\tilde{x}}_k \\approx \\frac{\\tilde{x}_{k+1}-\\tilde{x}_k}{\\Delta t}$, and then solve for $x_{k+1}$ as follows: \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\tilde{x}_{k+1}-\\tilde{x}_k}{\\Delta t} &= A\\tilde{x}_k+B\\tilde{u}_k \\\\\n",
    "\\tilde{x}_{k+1} &= \\tilde{x}_k + \\Delta t\\left(A\\tilde{x}_k+B\\tilde{u}_k\\right) \\\\\n",
    "&= (I+\\Delta tA)\\tilde{x}_k + (\\Delta tB)\\tilde{u}_k .\n",
    "\\end{align}\n",
    "\n",
    "This gives us the discretized version:\n",
    "$$\n",
    "\\tilde{x}_{k+1}=A_d\\tilde{x}_k+B_d\\tilde{u}_k.\n",
    "$$\n",
    "\n",
    "or the same equation, but expressed without tilde variables:\n",
    "$$\n",
    "x_{k+1}=A_d(x_k-x_{eq})+B_d(u_k-u_{eq})+x_{eq}.\n",
    "$$\n",
    "\n",
    "We could leave the discrete model in terms of the tilde variables; however, it is simpler to use the original state and input so that the reference trajectory and constraints do not have to be expressed relative to tilde variables. Also, this way the optimization solves directly for the actual inputs ($u_k$) so that the control is done without having to add the equilibrium input back to the output of our MPC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Horizon\n",
    "\n",
    "There are 2 pieces of information that affect how far into the future a state trajectory will be predicted: the time step ($t_s$) and the number of steps ($N$, labeled as $p$ in the image above from Wikipedia, and as $H$ in much of the literature on MPC).\n",
    "\n",
    "The length of the horizon is then $t_s*N$. So if we look 10 steps into the future and discretize the system at 0.1s, then the length of the horizon will be $0.1*10=1s$. Both of these values in the time horizon are tuning parameters for our controller. \n",
    "\n",
    "#### Time Horizon Length\n",
    "\n",
    "The effect of the length of the time horizon depends significantly on the system and the objective function defined. However, for many systems, a very small time horizon that does not initially include the system reaching steady state still works. \n",
    "\n",
    "The overall effect of the time horizon length is that if it is too short, our control results in a greedy behavior. For some systems this is acceptable, but if a system has significant momentum and the effect of this momentum is not predicted with enough time to react with control, the system will overshoot.\n",
    "MPC with an accurate model and a long enough time horizon that sees the system reach steady state can prevent overshoot while maintaining a fast response, it is optimal after all! Shorter time horizons can be provably sub-optimal, however, they still often result in effective control (more effective than LQR and PID). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Future States\n",
    "\n",
    "One iteration of MPC requires knowledge of the current state, $x_0$, and the optimization solver will generate an input trajectory, $u_0,u_1,...,u_N$, that the model will use to predict a state trajectory, $x_1,x_2,...,x_{N+1}$.\n",
    "Let's visually walk through a couple of steps.\n",
    "\n",
    "$$\n",
    "x_1 = A_d x_0 + B_d u_0.\n",
    "$$\n",
    "\n",
    "Here, $x_0$ needs to be known and provided to the optimization solver to produce $u_0$.\n",
    "Now the next step:\n",
    "\n",
    "$$\n",
    "x_2 = A_d x_1 + B_d u_1.\n",
    "$$\n",
    "\n",
    "We can solve the first equation for $x_1$ and the optimization solver will use that to generate $u_1$.\n",
    "You can see how this second equation could also be written entirely in terms of $x_0$ by substituting the first equation into the second:\n",
    "\n",
    "\\begin{align}\n",
    "x_2 &= A_d (A_d x_0 + B_d u_0) + B_d u_1 \\\\\n",
    "    &= A_d^2 x_0 + A_d B_d u_0 + B_d u_1.\n",
    "\\end{align}\n",
    "\n",
    "Now let's solve for $x_3$:\n",
    "\n",
    "$$\n",
    "x_3 = A_d x_2 + B_d u_2.\n",
    "$$\n",
    "\n",
    "Let's do a similar substitution as before:\n",
    "\n",
    "\\begin{align}\n",
    "x_3 &= A_d (A_d^2 x_0 + A_d B_d u_0 + B_d u_1) + B_d u_2 \\\\\n",
    "    &= A_d^3 x_0 + A_d^2 B_d u_0 + A_d B_d u_1 + B_d u_2.\n",
    "\\end{align}\n",
    "\n",
    "Continuing this pattern we can solve for $x_n$ in terms of only $x_0$, $A_d$, $B_d$, and the inputs.\n",
    "\n",
    "$$\n",
    "x_N = A_d^N x_0 + \\sum_{j=0}^{N-1} A_d^{N-1-j} B_d u_j.\n",
    "$$\n",
    "\n",
    "We know $x_0$, $A_d$, and $B_d$ (which are constant over the horizon because it is an LTI model).\n",
    "So the only real unknowns are all of the inputs in the trajectory, which are used to solve for future states (the future state trajectory can be an optimization variable or it can be hidden internally in the optimization, which is the difference between what is formally the intrinsic and extrinsic optimzation formulation (explained in more depth at the end of this notebook). The biggest takeaway for now is that the optimization only needs to know the current state $x_0$ and the commanded reference trajectory in order to generate an optimal input trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "To this point, we have talked about how the optimization solver generates a trajectory of inputs to produce a desirable trajectory of outputs. Let's take a look at how the optimization does that. There are 3 main pieces to the optimization problem: the cost function, constraints, and parameters.\n",
    "\n",
    "#### Cost Function\n",
    "\n",
    "As with any optimization problem (and similar to LQR), we need to come up with a function to minimize.\n",
    "This function can have several names, such as objective function or loss function.\n",
    "With MPC, it is usually referred to as the cost function.\n",
    "In general with controls, we want to minimize error.\n",
    "Error is defined as the difference in our desired state and current state.\n",
    "\n",
    "As the focus of this tutorial is an easy-to-solve MPC problem, and in order to have a global minimum, we want the cost function to be quadratic.\n",
    "For scalars, a quadratic function is $f(x)=\\alpha x^2$ where $\\alpha$ is positive.\n",
    "For vectors, a quadratic function looks like $f(\\mathbf{x})=\\mathbf{x}^TQ\\mathbf{x}$ where Q is a semi-positive definite matrix (but non-zero).\n",
    "Q is generally a diagonal matrix of scalar weights for each state.\n",
    "\n",
    "Therefore, here is a basic cost function that would minimize the error in states:\n",
    "\n",
    "$$\n",
    "J_x = \\sum_{k=1}^{N+1} (x_k - x_{des,k})^T Q (x_k - x_{des,k}).\n",
    "$$\n",
    "\n",
    "The cost function can really be any function of states and inputs.\n",
    "Sometimes, one might want to minimize the amount of energy used by the system (e.g. prolong battery life or reduce an excess of force or torque). This can be done by adding a quadratic cost on the inputs:\n",
    "\n",
    "$$\n",
    "J_u=\\sum_{k=0}^{N} u_k^T R u_k.\n",
    "$$\n",
    "\n",
    "If your inputs were motor commands to keep a multirotor vehicle in the air, you might want to add a cost on the difference in applied inputs and equilibrium inputs:\n",
    "\n",
    "$$\n",
    "J_u=\\sum_{k=0}^{N} (u_k - u_{eq})^T R (u_k - u_{eq}).\n",
    "$$\n",
    "\n",
    "Another common idea is to have conservative inputs that do not change very fast (I.E. not bang-bang control).\n",
    "This can be accomplished by adding a cost to the change in inputs from step to step:\n",
    "\n",
    "$$\n",
    "J_u=\\sum_{k=0}^{N-1} (u_{k+1} - u_k)^T R (u_{k+1} - u_k).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraints\n",
    "\n",
    "If all we had was a cost function with no constraints, the optimization would basically tell you that you could \"just teleport to the state where you want to go.\"\n",
    "Unfortunately, for any real system, we are constrained by the time and inputs it takes to move between one state and another. Our optimization therefore needs a constraint to describe how our system is capable of moving.\n",
    "The most important constraint, then, is the model that we came up with for the dynamic system:\n",
    "\n",
    "$$\n",
    "x_{k+1} = A_d x_k + B_d u_k \\quad \\forall k \\in 0,1,...,N.\n",
    "$$\n",
    "\n",
    "So what are some other constraints/costs we might want to use?\n",
    "\n",
    "We already saw one example in our discussion on cost functions: don't let inputs change very much from step to step. Although we used this as an example cost, we can also explicitly limit the change in inputs from step to step as follows:\n",
    "\n",
    "$$\n",
    "-\\Delta u_{max} \\le u_{k+1} - u_k \\le \\Delta u_{max}.\n",
    "$$\n",
    "\n",
    "You should now be able to see that some outcomes we want can be achieved as either a constraint or a cost, depending on how strictly we want to accomplish that outcome. And if we ever violate a constraint, a common practice is to push constraints to a term in our cost function until our system is feasible again (however, this sometimes means we are using a constraint where we should not - since it can be violated). \n",
    "\n",
    "Another common constraint is the saturation limits for states and inputs. Perhaps a state is a robot arm joint angle that can only move between a specific range of angles. Or maybe you have an input of thrust from a propellor that has a physical max and can't be negative.\n",
    "\n",
    "These saturation limits can be provided to the optimization solver so that it theoretically knows how to provide optimal inputs that are dynamically achievable.\n",
    "\n",
    "Here is what the saturation constraints would look like:\n",
    "\n",
    "$$x_{min} \\le x_k \\le x_{max}$$\n",
    "\n",
    "$$u_{min} \\le u_k \\le u_{max}$$\n",
    "\n",
    "Despite only listing a few constraints, it should be clear that any constraint can be included as long as it is a function of states and inputs. In order to make a tractable optimization for MPC, these funtions will most often be linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "Parmeters required by our MPC optimization are fairly straightforward. They are values we provide to the optimization solver that stay constant for the solution found at a single instant in time (but calculating cost by propagating our model forward over the given time horizon). After the first input is applied, the optimization will be solved again and can/will have new values for the parameters.\n",
    "\n",
    "Examples are $A,B,N,x_0,x_{min},x_{max},u_{min},u_{max},$ and any others you might have specified in your cost function or constraints.\n",
    "\n",
    "$x_0$ is a value that will change almost every time, because the previous input you apply will likely cause you to move. If you are linearizing a nonlinear model then $A$ and $B$ can be updated at each time step as well, but they stay fixed for a single optimization with linear MPC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mass-Spring-Damper Example\n",
    "\n",
    "Below we show the necessary steps and example code to set up an MPC problem and solve it in the control loop. The focus here is not making this optimization super time-efficient, but instead in demonstrating functionality and output from a model predictive controller. \n",
    "\n",
    "\n",
    "### Analytical Model\n",
    "\n",
    "The governing dynamic equations are the following: \n",
    "\n",
    "$$m\\ddot{z}+b\\dot{z}+kz=f$$\n",
    "\n",
    "The equilibrium point is:\n",
    "\n",
    "$$f_{eq}=kz_{eq}.$$\n",
    "\n",
    "Notice how the equilibrium force depends on the location of the mass. We don't have to worry too much about this in this example, because it is already a linear system, meaning we can linearize about any point and still model the system perfectly. Let us choose to use $z_{eq}=f_{eq}=0$ and not worry about these values to keep our example simple.\n",
    "\n",
    "Next we generate the state-variable form:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{dz}{dt}  &= \\dot{z} \\\\\n",
    "\\frac{d\\dot{z}}{dt} &= -\\frac{b}{m}\\dot{z} - \\frac{k}{m}z + \\frac{1}{m}f\n",
    "\\end{align}\n",
    "\n",
    "Let $\\mathbf{x}=[z, \\dot{z}]^T$ and $u=[f]$, then the state-space form is the following:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\dot{x}} = \n",
    "\\begin{bmatrix} 0 & 1 \\\\ -\\frac{k}{m} & -\\frac{b}{m} \\end{bmatrix}\n",
    "\\mathbf{x} +\n",
    "\\begin{bmatrix} 0 \\\\ \\frac{1}{m} \\end{bmatrix} u\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Setup\n",
    "\n",
    "For the code example shown below for the mass-spring-damper system, we will minimize tracking error as well as minimize the inputs directly. The formal optimization problem can thus be written as such:\n",
    "\n",
    "\\begin{align}\n",
    "\\min_{u_k} \\quad& J = \\sum_{k=0}^N \\left\\lVert x_{k+1} - x_{des} \\right\\rVert_Q^2  + \\left\\lVert u_k \\right\\rVert_R^2 \\\\\n",
    "\\text{subject to:} \\nonumber \\\\\n",
    " \\quad& x_{k+1} = A_d x_k + B_d u_k \\\\\n",
    "& u_{min} \\leq u_k \\leq u_{max}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "For the example below, we recommend you run all cells first, then play with the values for $Q$ and $R$ in the last cell. However, you may obviously examine other cells and change variables like $N$ (which is the length of the time horizon). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install needed modules that aren't always installed by default\n",
    "!pip3 install control\n",
    "!pip3 install cvxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed modules\n",
    "import numpy as np\n",
    "import control as ctrl\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time as now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MassSpringDamper:\n",
    "  def __init__(self, m, k, b, ts):\n",
    "    '''\n",
    "    m: mass, k: spring constant, b: damping coefficient\n",
    "    ts: discretization time step\n",
    "    '''\n",
    "    # continuous model\n",
    "    A = np.array([[0,1],[-k/m,-b/m]])\n",
    "    B = np.array([[0],[1/m]])\n",
    "    C = np.eye(2)\n",
    "    D = np.zeros((C.shape[0],1))\n",
    "\n",
    "    # define a continuous-time system using the state-space representation\n",
    "    sys = ctrl.ss(A,B,C,D)\n",
    "\n",
    "    # discretize model and extract A_d and B_d\n",
    "    sys_d = ctrl.c2d(sys, ts)\n",
    "    self.Ad = np.array(sys_d.A)\n",
    "    self.Bd = np.array(sys_d.B)\n",
    "\n",
    "  def propagate_dynamics(self, current_state, input):\n",
    "    # with a linear system you can integrate using the discretized matrices\n",
    "    # for a nonlinear system you would probably do RK4 integration\n",
    "    next_state = self.Ad @ current_state + self.Bd @ input\n",
    "    return next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "# This MPC class is very specific to cvxpy, which is quite slow :/\n",
    "# Howevver, it is a good example of how to set up an MPC problem\n",
    "# and should be easy see the terms and to adapt to other solvers. \n",
    "\n",
    "class MPC:\n",
    "  def __init__(self, Ad, Bd, x_lin, u_lin, x_des, N, Q, R):\n",
    "    '''\n",
    "    Ad and Bd: discretized state-space model\n",
    "    x_lin and u_lin: state and input to linearize about\n",
    "    x_des: desired/goal state\n",
    "    N: num steps in horizon\n",
    "    Q: state weights, R: input weights -> both must be 2D numpy arrays\n",
    "    '''\n",
    "    n,m = Bd.shape  # defining # of states (n) and inputs (m)\n",
    "    self.x = cp.Variable((n, N+1)) # add 1 for x0\n",
    "    self.u = cp.Variable((m, N))\n",
    "    self.x0 = cp.Parameter(n)\n",
    "    cost = 0\n",
    "    constr = [self.x[:,0] == self.x0] # don't allow optimization to change x0\n",
    "    u_limit = np.array([0,3])\n",
    "\n",
    "    # looping over each step in the time horizon from k=0 to N-1 \n",
    "    for k in range(N):\n",
    "      cost += cp.quad_form(self.x[:,k+1]-x_des, Q) # cost on error in states\n",
    "      cost += cp.quad_form(self.u[:,k], R) # cost on input\n",
    "      constr += [self.u[:,k] >= u_limit[0], self.u[:,k] <=u_limit[1]] # saturate inputs\n",
    "\n",
    "      # add model as dynamics constraint\n",
    "      x_tilde = self.x[:,k] - x_lin\n",
    "      u_tilde = self.u[:,k] - u_lin\n",
    "      constr += [self.x[:,k+1] == Ad @ x_tilde + Bd @ u_tilde + x_lin]\n",
    "      \n",
    "    self.problem = cp.Problem(cp.Minimize(cost), constr)\n",
    "  \n",
    "  def calc_next_u(self, xk):\n",
    "    self.x0.value = xk # set x0 to be current state\n",
    "    self.problem.solve(warm_start=True) # solve optimization\n",
    "    return self.u[:,0].value # return first input from trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(time, pos, inputs, x_des):\n",
    "  px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
    "  plt.rcParams['figure.figsize'] = (1000*px, 400*px)\n",
    "  plt.close('all')\n",
    "  tf = time[-1]\n",
    "\n",
    "  fig, ax = plt.subplots(1,2)\n",
    "  ax[0].plot(time, pos, 'b', label='x')\n",
    "  ax[0].plot([0,tf],[1,1]*x_des.item(0), 'r--', label='$x_{des}$')\n",
    "  ax[0].set_xlabel('Time (s)')\n",
    "  ax[0].set_ylabel('Position (m)')\n",
    "  ax[0].set_title('Mass Spring Damper Simulation')\n",
    "  ax[0].legend(loc='lower right')\n",
    "\n",
    "  ax[1].plot(time[1:], inputs, 'g', label='u')\n",
    "  ax[1].plot([0,tf],[0,0], 'c:', label='$u_{min}$')\n",
    "  ax[1].plot([0,tf],[3,3], color='orange', linestyle=':', label='$u_{max}$')\n",
    "  ax[1].set_xlabel('Time (s)')\n",
    "  ax[1].set_ylabel('Force (N)')\n",
    "  ax[1].set_title('Inputs')\n",
    "  ax[1].legend()\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim(Q, R):\n",
    "  ## initial condition and goal\n",
    "  x = np.array([0,0])\n",
    "  x_des = np.array([1,0])\n",
    "\n",
    "  ## simulation parameters\n",
    "  tf = 10\n",
    "  t = 0\n",
    "\n",
    "  ## plotting variables\n",
    "  pos = [x[0]]\n",
    "  inputs = []\n",
    "  time = [t]\n",
    "\n",
    "  ## simulated system\n",
    "  m,k,b = 5.0, 3.0, 0.5\n",
    "  ts = 0.05\n",
    "  system = MassSpringDamper(m, k, b, ts)\n",
    "\n",
    "  ## set up controller\n",
    "  Ad, Bd = system.Ad, system.Bd # assuming perfect model\n",
    "  x_lin = np.zeros(2) # linearize about origin\n",
    "  u_lin = np.zeros(1)\n",
    "\n",
    "  N = 10 # steps in horizon\n",
    "  controller = MPC(Ad, Bd, x_lin, u_lin, x_des, N, Q, R)\n",
    "\n",
    "  ## simulate\n",
    "  begin = now()\n",
    "  while t <= tf:\n",
    "    optimal_u = controller.calc_next_u(x)\n",
    "    inputs.append(optimal_u)\n",
    "      \n",
    "    x = system.propagate_dynamics(x, optimal_u)\n",
    "    pos.append(x[0])\n",
    "      \n",
    "    t += ts\n",
    "    time.append(t)\n",
    "\n",
    "  end = now()\n",
    "  print('elapsed time:\\t', end-begin)\n",
    "  print('final position:\\t', x[0])\n",
    "  print('final input:\\t', optimal_u[0])\n",
    "\n",
    "  plot_results(time, pos, inputs, x_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import control as ctrl\n",
    "# choose your own gains\n",
    "position_weight = 0\n",
    "velocity_weight = 0\n",
    "force_weight = 0\n",
    "\n",
    "# Don't change this code\n",
    "Q = np.diag([position_weight, velocity_weight])\n",
    "R = np.diag([force_weight])\n",
    "run_sim(Q, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Material About Formulating and Solving MPC \n",
    "\n",
    "### Explicit vs Implicit Optimization for MPC\n",
    "\n",
    "Traditionally, MPC uses both the states and inputs over the time horizon as optimization variables with the discrete-time model constraining the relationship between them, which is implicit MPC.\n",
    "Another option is to have the states be internal variables to the optimization or not even use them at all.\n",
    "It turns out that the dynamics constraint can be implicitly included in the cost function.\n",
    "This is because of the recursive nature of linear state-space models - only the initial state and trajectory of inputs are needed to calculate the future trajectory of states as we saw earlier:\n",
    "$$\n",
    "x_N = A^N x_0 + \\sum_{j=0}^{N-1} A^{N-1-j} B u_j.\n",
    "$$\n",
    "\n",
    "So we could write a single function $g$ that takes in the current state and all of the inputs over the horizon and output a vector of the entire horizon of states concatenated together, which changes the cost function to this:\n",
    "$$\n",
    "J_x = \\left\\lVert g(x_0, u_0, \\cdots, u_N) - x_{ref} \\right\\rVert_Q.\n",
    "$$\n",
    "\n",
    "We have found that with many modern solvers, this implicit formulation of MPC is faster (see https://arxiv.org/abs/2001.04931).\n",
    "\n",
    "\n",
    "### Parameterized Inputs for MPC (to reduce size of optimization)\n",
    "\n",
    "In general with optimization, the more design variables there are the longer it takes to solve the problem. 30 design variables is often considered a large optimization problem. \n",
    "\n",
    "Now consider the system you want to work with. For example, a quadcopter has 9 states and 4 inputs.\n",
    "Even with a horizon of 10 steps, this leads to $90 + 40 = 130$ design variables. Using implicit MPC cuts out the 90, which might be why it solves faster, but there are still 40 design variables left for this small time horizon.\n",
    "\n",
    "This input space can be parameterized to significantly reduce the search space and solve time.\n",
    "This trick again came from Phil Hyatt's PhD work: he let the optimization choose 3 sets of inputs (one at the beginning, middle, and end of the horizon) and then linearly interpolated between these for the rest of the inputs in the trajectory. For the quadcopter, this would result in only $4*3 = 12$ design variables, which is a much smaller search space. You can use more than 3 sets of inputs; however, around 4-5 was shown to basically have the same performance as not parameterizing. \n",
    "\n",
    "This also implicitly adds a slew rate constraint to some degree, which is often beneficial for smooth input trajectories that tend to be a little more conservative. Initially, this might seem like a bad idea because it seems like the optimization has less freedom; however, we have shown that there is almost an insignificant reduction of overall performance with the controller. \n",
    "\n",
    "A key piece to understand is that the first input is unconstrained (the input to apply now) and is unaffected from the previous optimization, so the system is free to move however it wants if the first input is only ever applied before optimizing again.\n",
    "\n",
    "One thing to note is that the parameterization is still discretly sampled, meaning that the input signal is not actually continuous. More complicated parameterizations may result in additional improvements, but this simple linear interpolation is surprisingly performant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
